{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block Six Analytics Technical Challenge\n",
    "### Bobby Shi\n",
    "Thanks for the opportunity to go through the interview process for Block Six.  This was a really interesting challenge and I learned not only a lot about the basics of machine learning, but also simple stuff like how to make good code design and how to make my code run faster.  A lot of time was spent simply researching what a lot of the terms meant and understanding the nuances of the code.  All in all, it was a fantastic learning experience and something I really benefited from even beyond the scope of the interview process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from IPython.display import display\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.sparse import csr_matrix, hstack, lil_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I import the packages I need.  I import the Counter object in order to obtain a tally of how many times each word appears in the tweets.  I import display simply so the dataframe in the final print looks pretty.  From nltk.corpus I import stopwords to be removed and wordnet as a lexicon of English words for a specific cleaning process (removing repeat letters from words).  I import PorterStemmer to stem words, but this will be an option that can be set.  word_tokenize is obviously to tokenize words.  From scipy.sparse I import csr_matrix and lil_matrix so I can create a matrix of 1.6 million rows and thousands of columns of zeros and ones (mostly zeros) without hogging all of my memory &mdash; this creates a sparse matrix; hstack is to append columns to the matrix.  Numpy is used for math, pandas is used to read data, and regex is used to clean tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILE = 'training.1600000.processed.noemoticon.csv'\n",
    "WHITELIST = ['not']\n",
    "MIN_OCCURENCES = 5\n",
    "STEM = False\n",
    "RATE = 0.001\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some options that can be set.  FILE is obviously the data file.  WHITELIST indicates words that will not be removed that otherwise would be a stopword.  MIN_OCCURENCES is the minimum number of times a word must appear in all of the tweets in order to be counted in the regression.  STEM is whether or not to stem.  RATE represents the initial learning reate.  EPOCHS is how many times we will conduct a full pass through the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sentiment:\n",
    "    \n",
    "    def __init__(self, sentiments, tweets):\n",
    "        '''\n",
    "        Initialize the object with certain attributes\n",
    "        '''\n",
    "        \n",
    "        # check if the number of tweets is equal to the number of sentiment scores\n",
    "        if len(tweets) != len(sentiments):\n",
    "            return -1\n",
    "        \n",
    "        self.tweets = tweets\n",
    "        self.sentiments = sentiments\n",
    "                \n",
    "        self.wordlist = None\n",
    "        \n",
    "        self.stemmer = PorterStemmer()\n",
    "                \n",
    "        self.w = None\n",
    "        \n",
    "        print('Number of tweets: {}'.format(len(tweets)))\n",
    "        \n",
    "    @staticmethod\n",
    "    def remove_re(tweet, regexp):\n",
    "        '''\n",
    "        Executes the basic cleaning methods (remove urls, numbers, symbols, etc.)\n",
    "        '''\n",
    "        tweet = re.sub(regexp, '', tweet)\n",
    "        return tweet\n",
    "        \n",
    "    def tokenize(self, tweet, whitelist=WHITELIST, stem=STEM):\n",
    "        '''\n",
    "        Takes a tweet and cleans, tokenizes, removes stopwords, and stems\n",
    "        '''\n",
    "        \n",
    "        # calls the static method to clean the Tweet\n",
    "        tweet = Sentiment.remove_re(tweet, re.compile(r'\\w+:\\/\\/\\S+'))\n",
    "        tweet = Sentiment.remove_re(tweet, re.compile(r'[^0-9A-Za-z \\t]'))\n",
    "        tweet = Sentiment.remove_re(tweet, re.compile(r'@[A-Za-z0-9]+'))\n",
    "        tweet = Sentiment.remove_re(tweet, re.compile(r'^\\d+\\s|\\s\\d+\\s|\\s\\d+$'))\n",
    "       \n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        # tokenize\n",
    "        tokens = word_tokenize(tweet)\n",
    "        \n",
    "        # removes repeat letters from words and checks with dictionary to see if word exists\n",
    "        for token in tokens:\n",
    "            if re.search(r'(.)\\1\\1', token):\n",
    "                token2 = re.sub(r'(.)\\1+', r'\\1\\1', token)\n",
    "                if wordnet.synsets(token2):\n",
    "                    tokens[:] = [t.replace(token, token2) for t in tokens]\n",
    "                else:\n",
    "                    token3 = re.sub(r'(.)\\1+', r'\\1', token2)\n",
    "                    tokens[:] = [t.replace(token, token3) for t in tokens]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        # remove stopwords\n",
    "        stop = [word for word in stopwords.words('english') if word not in whitelist]\n",
    "        tokens[:] = [token for token in tokens if token not in stop]\n",
    "        \n",
    "        if stem:\n",
    "            # stem\n",
    "            tokens[:] = [self.stemmer.stem(token) for token in tokens]\n",
    "            return tokens\n",
    "        else:\n",
    "            return tokens\n",
    "    \n",
    "    def create_wordlist(self, min_occurences=MIN_OCCURENCES):\n",
    "        '''\n",
    "        Return wordlist if it already exists or create it\n",
    "        '''\n",
    "        if self.wordlist is not None:\n",
    "            return self.wordlist\n",
    "        \n",
    "        else:\n",
    "            all_words = Counter()\n",
    "            \n",
    "            for i in range(len(self.tweets)):\n",
    "                tokens = self.tokenize(self.tweets[i])\n",
    "                all_words.update(tokens)\n",
    "                self.tweets[i] = tokens\n",
    "            \n",
    "            # only keep words that appear at least certain number of times\n",
    "            list_of_words = [word for word, count in all_words.items() if count >= min_occurences]\n",
    "            self.wordlist = {value: key for key, value in dict(enumerate(list_of_words)).items()}\n",
    "            \n",
    "            print('Number of words: {}'.format(len(self.wordlist)))\n",
    "            print('\\n')\n",
    "\n",
    "            return self.wordlist\n",
    "    \n",
    "    def words_matrix(self):\n",
    "        '''\n",
    "        Creates a sparse matrix with row representing tweets and columns representing presence of words\n",
    "        First column is all ones to indicate the bias term\n",
    "        '''\n",
    "        wordlist = self.create_wordlist()\n",
    "        \n",
    "        # creates a sparse matrix that can be sliced\n",
    "        init_matrix = lil_matrix((len(self.tweets), len(wordlist)), dtype='int8')\n",
    "        \n",
    "        # populate matrix\n",
    "        for i in range(len(self.tweets)):\n",
    "            distinct_words = self.tweets[i]\n",
    "            for word in distinct_words:\n",
    "                if word in wordlist:\n",
    "                    j = wordlist[word]\n",
    "                    init_matrix[i,j] = 1\n",
    "                else:\n",
    "                    pass\n",
    "        \n",
    "        # add column of ones to indicate presence of bias\n",
    "        ones = np.ones((len(self.tweets), 1), dtype='int8')\n",
    "        \n",
    "        # convert to csr matrix\n",
    "        matrix = hstack([ones, init_matrix]).tocsr()\n",
    "        \n",
    "        n, m = matrix.shape\n",
    "        print('The boolean matrix has {} rows and {} columns.'.format(n, m))\n",
    "        print('\\n')\n",
    "        \n",
    "        return matrix\n",
    "    \n",
    "    def optimize(self, initial_rate=RATE, epochs=EPOCHS):\n",
    "        '''\n",
    "        Set initial weights and execute the gradient descent process for the specified number of epochs\n",
    "        '''\n",
    "        # sparse matrix\n",
    "        x = self.words_matrix()\n",
    "        n, m = x.shape\n",
    "        \n",
    "        # sentiment scores - should match number of tweets\n",
    "        y = np.array(self.sentiments)\n",
    "        \n",
    "        # set initial bias\n",
    "        y_bar = np.mean(y)\n",
    "        b_zero = np.exp(y_bar) / 1.0 + np.exp(y_bar)\n",
    "        \n",
    "        # set initial weight vector - bias followed by zeros\n",
    "        self.w = np.append(np.array([b_zero]), np.zeros(m - 1))\n",
    "         \n",
    "        previous_cost = 0    \n",
    "            \n",
    "        # execute gradient descent\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            if epoch != 0:\n",
    "                print('Shuffling...')\n",
    "                print('\\n')\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "            # shuffle dataset\n",
    "            indices = np.random.permutation(n)\n",
    "            x, y = x[indices,:], y[indices]\n",
    "            \n",
    "            print('Pass {} through data...'.format(epoch + 1))\n",
    "            \n",
    "            stochastic(self.w, x, y, initial_rate, epoch)\n",
    "            \n",
    "            cost_value = cost_function(self.w, x, y)\n",
    "            \n",
    "            print('Pass complete!')\n",
    "            print('Cost value {}: {}'.format(str(epoch + 1), cost_value))\n",
    "            \n",
    "            if epoch != 0:\n",
    "                print('Difference in cost: {}'.format(previous_cost - cost_value))\n",
    "                print('\\n')\n",
    "            else:\n",
    "                print('\\n')\n",
    "            \n",
    "            previous_cost = cost_value\n",
    "            \n",
    "        \n",
    "        return self.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What I wanted to do with the Sentiment class was to create something reproducible.  Disregarding the names of the variables, everything done here can largely be applied to other forms of sentiment analysis with features other than simply the words.  For example, the cleaning methods are fairly adaptable, and one can choose which methods to use in accordance with the dataset.  The wordlist and the matrix are easily extendable to include whatever features one might want to use for analysis, and the optimization process can reflect whatever process is chosen (I learned there's a lot, lot more than just regression techniques)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1.0 / (1.0 + np.exp(-1.0 * z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_function(w, x, y):\n",
    "    '''\n",
    "    w: weight vector\n",
    "    x: tweet matrix\n",
    "    y: sentiment vector\n",
    "    '''\n",
    "    n, m = x.shape\n",
    "    z = x.dot(w)\n",
    "    \n",
    "    cost = (1.0 / n) * ((-1.0 * y).dot(np.log(logistic(z))) - (1.0 - y).dot(np.log(1.0 - logistic(z))))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic(w, x, y, initial_rate, epoch):\n",
    "    '''\n",
    "    w: weight vector\n",
    "    x: tweet matrix\n",
    "    y: sentiment vector\n",
    "    rate: learning rate\n",
    "    '''\n",
    "    n, m = x.shape\n",
    "    rate = initial_rate\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        #adjust learning rate\n",
    "        rate = learning_rate(initial_rate, i, epoch)\n",
    "        \n",
    "        z = x[i].dot(w)[0]\n",
    "        nonzero = x[i].nonzero()[1]\n",
    "        for j in nonzero:\n",
    "            w[j] = w[j] - rate * (logistic(z) - y[i]) * x[i,j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous three functions are all math.  The first one is the logistic function, represented by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p=f(z)=\\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $p$ is the log odds of a tweet having positive sentiment.  In our analysis, we have that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z=b+\\sum_{1}^n w_ix_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where each $w_i$ indicates the weight of a certain word and $x_i$ indicates the presence of that word in the specific tweet ($b$ is the bias term)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function given in the email is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C(\\mathbf{w}\\mid\\mathbf{x}, y)=\n",
    "\\cases{\n",
    "-\\ln{\\left(\\frac{1}{1+e^{-\\left(b+\\sum_{1}^n w_ix_i\\right)}}\\right)} & y=1 \\\\\n",
    "-\\ln{\\left(1-\\frac{1}{1+e^{-\\left(b+\\sum_{1}^n w_ix_i\\right)}}\\right)} & y=0\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, for ease, I will allow $z=b+\\sum_{1}^n w_ix_i$.  Thus, for each tweet, we can simplify the above equation to be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C(\\mathbf{w}\\mid\\mathbf{x}, y)=-y\\ln{\\left(\\frac{1}{1+e^{-\\left(b+\\sum_{1}^n w_ix_i\\right)}}\\right)}-(1-y)\\ln{\\left(1-\\frac{1}{1+e^{-\\left(b+\\sum_{1}^n w_ix_i\\right)}}\\right)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, $y=0$ and $y=1$, the cost functions are the same.  Now, replace $n$ with $m$; I want to do this to be consistent with how I wrote my code.  Thus, $m$ is the number of distinct words, and $n$ is the number of tweets &mdash; in our case $n=1,600,000$.  Thus, we can write the total mean cost as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{1}{n}\\sum_{1}^{n} -y_j\\ln{\\left(\\frac{1}{1+e^{-\\left(b+\\sum_{1}^m w_ix_i\\right)}}\\right)}-(1-y_j)\\ln{\\left(1-\\frac{1}{1+e^{-\\left(b+\\sum_{1}^m w_ix_i\\right)}}\\right)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot products are really useful here.  I choose to allow $b=w_0$, and $x_0$ is always $1$; thus, we have that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$b+\\sum_{1}^m w_ix_i=\\sum_{0}^m w_ix_i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This term is dependent on tweet; thus, it is the $j^{th}$ entry in a $n\\times 1$ matrix.  This matrix is given by the dot product between our matrix $\\mathbf{x}$, which is $n\\times m$ (using the same $n$, $m$ as before), and the transpose of the weight matrix (vector) $\\mathbf{w}$, which is $m\\times 1$.  If we call this matrix $\\mathbf{z}$, we have that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{z}=\\mathbf{x}\\cdot \\mathbf{w}^T.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently, numpy allows us to perform operations termwise, so we can do the logistic function on the entire matrix.  Now notice that each $y_j$ is the $j^{th}$ entry of the sentiment vector $\\mathbf{y}$, which can be thought of as a $1\\times n$ matrix.  Thus, we can again do dot product, and we have that the mean cost is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "-\\frac{1}{n}(\\mathbf{y}\\cdot \\ln{(\\mathbf{z})}+(1-\\mathbf{y})\\cdot(\\ln{(1-\\mathbf{z})})).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what I coded into the function returning the cost value.  I call this function after each epoch to see if my cost value is decreasing after each pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent process is also given in the email, which is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w_{\\text{new}}\\leftarrow w_{\\text{old}} -\\gamma(p-y)x_i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, I iterated by row on my $n\\times m$ matrix.  To speed up my code, I simply returned the indices of nonzero values in each row, and then did the gradient descent step on those weights.  In this way, I didn't have to multiply by zero a bunch of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_rate(initial_rate, i, epoch):\n",
    "    '''\n",
    "    Adjust the learning rate\n",
    "    '''\n",
    "    if epoch == 0:\n",
    "        return initial_rate\n",
    "    else:\n",
    "        return initial_rate * (0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I return with the learning rate, which is called in the gradient descent function.  This is the area that was the most murky to me, and I talked with professors and read a few papers on the subject &mdash; and it is the area that I would most like to learn more about.  In the end, the general idea I had was to at some point reduce the learning rate as I approach the bottom of my cost function.  Fortunately, log loss is convex, so I don't need to worry about local minimums, and so the reduction would lower the chances of any oscillations about or even potential divergence away from the global minimum.  I experimented with a lot of different rates, including letting it stay 0.001 the whole time, dividing by the number of tweets passed through after each step (so at the end, it would've been $\\frac{0.001}{3.2\\text{ million}}$, and making it bigger at 0.01 and making it smaller in the same way (dividing by number of tweets already passed through).  In the end, I let it stay at 0.001 through the first pass and then reduce it to 0.00075 during the second pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(file=FILE):\n",
    "    \n",
    "    print('Reading data...')\n",
    "    \n",
    "    # load file\n",
    "    load = pd.read_csv(file, encoding=\"ISO-8859-1\", header=None)\n",
    "    \n",
    "    # preprocess a bit\n",
    "    load[0] = load[0].replace(to_replace=4, value=1)\n",
    "    df = load[[0, 5]]\n",
    "    df.columns = ['sentiment', 'tweet']\n",
    "    \n",
    "    print('Data loaded.')\n",
    "    print('\\n')\n",
    "    print('Head and tail of data:')\n",
    "    display(pd.concat([df.head(), df.tail()]))\n",
    "    print('\\n')\n",
    "    \n",
    "    # extract columns to lists\n",
    "    sentiments = df['sentiment'].tolist()\n",
    "    tweets = df['tweet'].tolist()\n",
    "    \n",
    "    # instantiate Sentiment object\n",
    "    data = Sentiment(sentiments, tweets)\n",
    "    w = data.optimize()\n",
    "    \n",
    "    # quickly reverse dictionary to be able to search by index\n",
    "    reversed_wordlist = {value: key for key, value in data.wordlist.items()}\n",
    "    \n",
    "    # get 5 largest weights\n",
    "    l_ind = np.argpartition(w, -5)[-5:]\n",
    "    l_sort = l_ind[np.argsort(w[l_ind])][::-1]\n",
    "    l_weights = w[l_sort]\n",
    "    l_tuples = list(zip(l_sort, l_weights))\n",
    "    \n",
    "    print('Top 5 weights:')\n",
    "    for tup in l_tuples:\n",
    "        print('{}: {}'.format(reversed_wordlist[tup[0]], tup[1]))\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    # get 5 smallest weights\n",
    "    s_ind = np.argpartition(w, 5)[:5]\n",
    "    s_sort = s_ind[np.argsort(w[s_ind])]\n",
    "    s_weights = w[s_sort]\n",
    "    s_tuples = list(zip(s_sort, s_weights))\n",
    "    \n",
    "    print('Bottom 5 weights:')\n",
    "    for tup in s_tuples:\n",
    "        print('{}: {}'.format(reversed_wordlist[tup[0]], tup[1]))\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    # print bias term\n",
    "    print('Bias: {}'.format(w[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the main function, I load data and preprocess it a bit.  The encoding is so I can read the file; without it resulted in an error.  I dropped all columns except for the sentiment values and the tweets, and for some reason positive sentiments were indicated by 4, so I changed all of it to 1.  Then I stored all the sentiment values and the tweets to lists.  I instantiate the Sentiment object and optimize it, returning weights, and then I print the top 5 weights, bottom 5 weights, and the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Data loaded.\n",
      "\n",
      "\n",
      "Head and tail of data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>1</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>1</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>1</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>1</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>1</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                              tweet\n",
       "0                0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1                0  is upset that he can't update his Facebook by ...\n",
       "2                0  @Kenichan I dived many times for the ball. Man...\n",
       "3                0    my whole body feels itchy and like its on fire \n",
       "4                0  @nationwideclass no, it's not behaving at all....\n",
       "1599995          1  Just woke up. Having no school is the best fee...\n",
       "1599996          1  TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997          1  Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998          1  Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599999          1  happy #charitytuesday @theNSPCC @SparksCharity..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of tweets: 1600000\n",
      "Number of words: 86645\n",
      "\n",
      "\n",
      "The boolean matrix has 1600000 rows and 86646 columns.\n",
      "\n",
      "\n",
      "Pass 1 through data...\n",
      "Pass complete!\n",
      "Cost value 1: 0.5165943596188627\n",
      "\n",
      "\n",
      "Shuffling...\n",
      "\n",
      "\n",
      "Pass 2 through data...\n",
      "Pass complete!\n",
      "Cost value 2: 0.5041239183073967\n",
      "Difference in cost: 0.012470441311466018\n",
      "\n",
      "\n",
      "Top 5 weights:\n",
      "letting: 1.7264024149960315\n",
      "hows: 1.5696658721225454\n",
      "carousella: 1.4464374513979492\n",
      "till: 1.2807266501718593\n",
      "hear: 1.2235996320188314\n",
      "\n",
      "\n",
      "Bottom 5 weights:\n",
      "ooh: -2.9092679938584163\n",
      "premiere: -2.0320494453670266\n",
      "aaw: -1.8920347712339727\n",
      "spent: -1.8807150852187302\n",
      "cameron: -1.7862225131754408\n",
      "\n",
      "\n",
      "Bias: 0.27194235724608484\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One takeaway from this project was how important it was to speed up my code in any way possible, especially through vectorizing stuff, from creating the matrices to doing dot product instead of using a Python for loop.  Additionally, when first creating this, there were *a lot* of words that only appeared one time &mdash; like 500,000 of them.  Getting rid of those was absolutely vital in speeding up the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my results, an interesting result was that negative words seemed to have beeh more negative than positive word were positive.  Additionally, no matter how I adjusted my learning rate, I never achieved a significant improvement (like on the order of 0.1) in cost, but this is probably by design.  This of course is somewhere I can improve.  Additionally, I can learn how to better clean and extract features &mdash; one area I wish I understood better is how to deal with negations, words like \"not\" and \"no\" especially.  For example, \"love\" is generally a positive word, but with \"not\" in front of it, like \"I do not love that\", the tweet becomes negative.  I also wish I could implement more features beyond just words.  All in all, it was a fantastic experience, and something I definitely fell in love with during the whole process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
