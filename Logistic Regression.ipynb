{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from IPython.display import display\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from scipy.sparse import csr_matrix, hstack, lil_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FILE = 'training.1600000.processed.noemoticon.csv'\n",
        "WHITELIST = ['not']\n",
        "MIN_OCCURENCES = 5\n",
        "STEM = False\n",
        "RATE = 0.001\n",
        "EPOCHS = 2"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sentiment:\n",
        "    \n",
        "    def __init__(self, sentiments, tweets):\n",
        "        '''\n",
        "        Initialize the object with certain attributes\n",
        "        '''\n",
        "        \n",
        "        # check if the number of tweets is equal to the number of sentiment scores\n",
        "        if len(tweets) != len(sentiments):\n",
        "            return -1\n",
        "        \n",
        "        self.tweets = tweets\n",
        "        self.sentiments = sentiments\n",
        "                \n",
        "        self.wordlist = None\n",
        "        \n",
        "        self.stemmer = PorterStemmer()\n",
        "                \n",
        "        self.w = None\n",
        "        \n",
        "        print('Number of tweets: {}'.format(len(tweets)))\n",
        "        \n",
        "    @staticmethod\n",
        "    def remove_re(tweet, regexp):\n",
        "        '''\n",
        "        Executes the basic cleaning methods (remove urls, numbers, symbols, etc.)\n",
        "        '''\n",
        "        tweet = re.sub(regexp, '', tweet)\n",
        "        return tweet\n",
        "        \n",
        "    def tokenize(self, tweet, whitelist=WHITELIST, stem=STEM):\n",
        "        '''\n",
        "        Takes a tweet and cleans, tokenizes, removes stopwords, and stems\n",
        "        '''\n",
        "        \n",
        "        # calls the static method to clean the Tweet\n",
        "        tweet = Sentiment.remove_re(tweet, re.compile(r'\\w+:\\/\\/\\S+'))\n",
        "        tweet = Sentiment.remove_re(tweet, re.compile(r'[^0-9A-Za-z \\t]'))\n",
        "        tweet = Sentiment.remove_re(tweet, re.compile(r'@[A-Za-z0-9]+'))\n",
        "        tweet = Sentiment.remove_re(tweet, re.compile(r'^\\d+\\s|\\s\\d+\\s|\\s\\d+$'))\n",
        "       \n",
        "        tweet = tweet.lower()\n",
        "        \n",
        "        # tokenize\n",
        "        tokens = word_tokenize(tweet)\n",
        "        \n",
        "        # removes repeat letters from words and checks with dictionary to see if word exists\n",
        "        for token in tokens:\n",
        "            if re.search(r'(.)\\1\\1', token):\n",
        "                token2 = re.sub(r'(.)\\1+', r'\\1\\1', token)\n",
        "                if wordnet.synsets(token2):\n",
        "                    tokens[:] = [t.replace(token, token2) for t in tokens]\n",
        "                else:\n",
        "                    token3 = re.sub(r'(.)\\1+', r'\\1', token2)\n",
        "                    tokens[:] = [t.replace(token, token3) for t in tokens]\n",
        "            else:\n",
        "                pass\n",
        "            \n",
        "        # remove stopwords\n",
        "        stop = [word for word in stopwords.words('english') if word not in whitelist]\n",
        "        tokens[:] = [token for token in tokens if token not in stop]\n",
        "        \n",
        "        if stem:\n",
        "            # stem\n",
        "            tokens[:] = [self.stemmer.stem(token) for token in tokens]\n",
        "            return tokens\n",
        "        else:\n",
        "            return tokens\n",
        "    \n",
        "    def create_wordlist(self, min_occurences=MIN_OCCURENCES):\n",
        "        '''\n",
        "        Return wordlist if it already exists or create it\n",
        "        '''\n",
        "        if self.wordlist is not None:\n",
        "            return self.wordlist\n",
        "        \n",
        "        else:\n",
        "            all_words = Counter()\n",
        "            \n",
        "            for i in range(len(self.tweets)):\n",
        "                tokens = self.tokenize(self.tweets[i])\n",
        "                all_words.update(tokens)\n",
        "                self.tweets[i] = tokens\n",
        "            \n",
        "            # only keep words that appear at least certain number of times\n",
        "            list_of_words = [word for word, count in all_words.items() if count >= min_occurences]\n",
        "            self.wordlist = {value: key for key, value in dict(enumerate(list_of_words)).items()}\n",
        "            \n",
        "            print('Number of words: {}'.format(len(self.wordlist)))\n",
        "            print('\\n')\n",
        "\n",
        "            return self.wordlist\n",
        "    \n",
        "    def words_matrix(self):\n",
        "        '''\n",
        "        Creates a sparse matrix with row representing tweets and columns representing presence of words\n",
        "        First column is all ones to indicate the bias term\n",
        "        '''\n",
        "        wordlist = self.create_wordlist()\n",
        "        \n",
        "        # creates a sparse matrix that can be sliced\n",
        "        init_matrix = lil_matrix((len(self.tweets), len(wordlist)), dtype='int8')\n",
        "        \n",
        "        # populate matrix\n",
        "        for i in range(len(self.tweets)):\n",
        "            distinct_words = self.tweets[i]\n",
        "            for word in distinct_words:\n",
        "                if word in wordlist:\n",
        "                    j = wordlist[word]\n",
        "                    init_matrix[i,j] = 1\n",
        "                else:\n",
        "                    pass\n",
        "        \n",
        "        # add column of ones to indicate presence of bias\n",
        "        ones = np.ones((len(self.tweets), 1), dtype='int8')\n",
        "        \n",
        "        # convert to csr matrix\n",
        "        matrix = hstack([ones, init_matrix]).tocsr()\n",
        "        \n",
        "        n, m = matrix.shape\n",
        "        print('The boolean matrix has {} rows and {} columns.'.format(n, m))\n",
        "        print('\\n')\n",
        "        \n",
        "        return matrix\n",
        "    \n",
        "    def optimize(self, initial_rate=RATE, epochs=EPOCHS):\n",
        "        '''\n",
        "        Set initial weights and execute the gradient descent process for the specified number of epochs\n",
        "        '''\n",
        "        # sparse matrix\n",
        "        x = self.words_matrix()\n",
        "        n, m = x.shape\n",
        "        \n",
        "        # sentiment scores - should match number of tweets\n",
        "        y = np.array(self.sentiments)\n",
        "        \n",
        "        # set initial bias\n",
        "        y_bar = np.mean(y)\n",
        "        b_zero = np.exp(y_bar) / 1.0 + np.exp(y_bar)\n",
        "        \n",
        "        # set initial weight vector - bias followed by zeros\n",
        "        self.w = np.append(np.array([b_zero]), np.zeros(m - 1))\n",
        "         \n",
        "        previous_cost = 0    \n",
        "            \n",
        "        # execute gradient descent\n",
        "        for epoch in range(epochs):\n",
        "            \n",
        "            if epoch != 0:\n",
        "                print('Shuffling...')\n",
        "                print('\\n')\n",
        "            else:\n",
        "                pass\n",
        "                \n",
        "            # shuffle dataset\n",
        "            indices = np.random.permutation(n)\n",
        "            x, y = x[indices,:], y[indices]\n",
        "            \n",
        "            print('Pass {} through data...'.format(epoch + 1))\n",
        "            \n",
        "            stochastic(self.w, x, y, initial_rate, epoch)\n",
        "            \n",
        "            cost_value = cost_function(self.w, x, y)\n",
        "            \n",
        "            print('Pass complete!')\n",
        "            print('Cost value {}: {}'.format(str(epoch + 1), cost_value))\n",
        "            \n",
        "            if epoch != 0:\n",
        "                print('Difference in cost: {}'.format(previous_cost - cost_value))\n",
        "                print('\\n')\n",
        "            else:\n",
        "                print('\\n')\n",
        "            \n",
        "            previous_cost = cost_value\n",
        "            \n",
        "        \n",
        "        return self.w"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic(z):\n",
        "    return 1.0 / (1.0 + np.exp(-1.0 * z))"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(w, x, y):\n",
        "    '''\n",
        "    w: weight vector\n",
        "    x: tweet matrix\n",
        "    y: sentiment vector\n",
        "    '''\n",
        "    n, m = x.shape\n",
        "    z = x.dot(w)\n",
        "    \n",
        "    cost = (1.0 / n) * ((-1.0 * y).dot(np.log(logistic(z))) - (1.0 - y).dot(np.log(1.0 - logistic(z))))\n",
        "    \n",
        "    return cost"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic(w, x, y, initial_rate, epoch):\n",
        "    '''\n",
        "    w: weight vector\n",
        "    x: tweet matrix\n",
        "    y: sentiment vector\n",
        "    rate: learning rate\n",
        "    '''\n",
        "    n, m = x.shape\n",
        "    rate = initial_rate\n",
        "    \n",
        "    for i in range(n):\n",
        "        \n",
        "        #adjust learning rate\n",
        "        rate = learning_rate(initial_rate, i, epoch)\n",
        "        \n",
        "        z = x[i].dot(w)[0]\n",
        "        nonzero = x[i].nonzero()[1]\n",
        "        for j in nonzero:\n",
        "            w[j] = w[j] - rate * (logistic(z) - y[i]) * x[i,j]"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous three functions are all math.  The first one is the logistic function, represented by"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$p=f(z)=\\frac{1}{1+e^{-z}}$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "where $p$ is the log odds of a tweet having positive sentiment.  In our analysis, we have that"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$z=b+\\sum_{1}^n w_ix_i$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "where each $w_i$ indicates the weight of a certain word and $x_i$ indicates the presence of that word in the specific tweet ($b$ is the bias term)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cost function given in the email is "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "C(\\mathbf{w}\\mid\\mathbf{x}, y)=\n",
        "\\cases{\n",
        "-\\ln{\\left(\\frac{1}{1+e^{-\\left(b+\\sum_{1}^n w_ix_i\\right)}}\\right)} & y=1 \\\\\n",
        "-\\ln{\\left(1-\\frac{1}{1+e^{-\\left(b+\\sum_{1}^n w_ix_i\\right)}}\\right)} & y=0\n",
        "}\n",
        "$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, for ease, I will allow $z=b+\\sum_{1}^n w_ix_i$.  Thus, for each tweet, we can simplify the above equation to be"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "C(\\mathbf{w}\\mid\\mathbf{x}, y)=-y\\ln{\\left(\\frac{1}{1+e^{-\\left(b+\\sum_{1}^n w_ix_i\\right)}}\\right)}-(1-y)\\ln{\\left(1-\\frac{1}{1+e^{-\\left(b+\\sum_{1}^n w_ix_i\\right)}}\\right)}.\n",
        "$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In both cases, $y=0$ and $y=1$, the cost functions are the same.  Now, replace $n$ with $m$; I want to do this to be consistent with how I wrote my code.  Thus, $m$ is the number of distinct words, and $n$ is the number of tweets &mdash; in our case $n=1,600,000$.  Thus, we can write the total mean cost as"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\frac{1}{n}\\sum_{1}^{n} -y_j\\ln{\\left(\\frac{1}{1+e^{-\\left(b+\\sum_{1}^m w_ix_i\\right)}}\\right)}-(1-y_j)\\ln{\\left(1-\\frac{1}{1+e^{-\\left(b+\\sum_{1}^m w_ix_i\\right)}}\\right)}.\n",
        "$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dot products are really useful here.  I choose to allow $b=w_0$, and $x_0$ is always $1$; thus, we have that"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$b+\\sum_{1}^m w_ix_i=\\sum_{0}^m w_ix_i.$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This term is dependent on tweet; thus, it is the $j^{th}$ entry in a $n\\times 1$ matrix.  This matrix is given by the dot product between our matrix $\\mathbf{x}$, which is $n\\times m$ (using the same $n$, $m$ as before), and the transpose of the weight matrix (vector) $\\mathbf{w}$, which is $m\\times 1$.  If we call this matrix $\\mathbf{z}$, we have that"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\mathbf{z}=\\mathbf{x}\\cdot \\mathbf{w}^T.$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conveniently, numpy allows us to perform operations termwise, so we can do the logistic function on the entire matrix.  Now notice that each $y_j$ is the $j^{th}$ entry of the sentiment vector $\\mathbf{y}$, which can be thought of as a $1\\times n$ matrix.  Thus, we can again do dot product, and we have that the mean cost is"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "-\\frac{1}{n}(\\mathbf{y}\\cdot \\ln{(\\mathbf{z})}+(1-\\mathbf{y})\\cdot(\\ln{(1-\\mathbf{z})})).\n",
        "$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is exactly what I coded into the function returning the cost value.  I call this function after each epoch to see if my cost value is decreasing after each pass."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gradient descent process is also given in the email, which is"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$w_{\\text{new}}\\leftarrow w_{\\text{old}} -\\gamma(p-y)x_i.$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, I iterated by row on my $n\\times m$ matrix.  To speed up my code, I simply returned the indices of nonzero values in each row, and then did the gradient descent step on those weights.  In this way, I didn't have to multiply by zero a bunch of times."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def learning_rate(initial_rate, i, epoch):\n",
        "    '''\n",
        "    Adjust the learning rate\n",
        "    '''\n",
        "    if epoch == 0:\n",
        "        return initial_rate\n",
        "    else:\n",
        "        return initial_rate * (0.75)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(file=FILE):\n",
        "    \n",
        "    print('Reading data...')\n",
        "    \n",
        "    # load file\n",
        "    load = pd.read_csv(file, encoding=\"ISO-8859-1\", header=None)\n",
        "    \n",
        "    # preprocess a bit\n",
        "    load[0] = load[0].replace(to_replace=4, value=1)\n",
        "    df = load[[0, 5]]\n",
        "    df.columns = ['sentiment', 'tweet']\n",
        "    \n",
        "    print('Data loaded.')\n",
        "    print('\\n')\n",
        "    print('Head and tail of data:')\n",
        "    display(pd.concat([df.head(), df.tail()]))\n",
        "    print('\\n')\n",
        "    \n",
        "    # extract columns to lists\n",
        "    sentiments = df['sentiment'].tolist()\n",
        "    tweets = df['tweet'].tolist()\n",
        "    \n",
        "    # instantiate Sentiment object\n",
        "    data = Sentiment(sentiments, tweets)\n",
        "    w = data.optimize()\n",
        "    \n",
        "    # quickly reverse dictionary to be able to search by index\n",
        "    reversed_wordlist = {value: key for key, value in data.wordlist.items()}\n",
        "    \n",
        "    # get 5 largest weights\n",
        "    l_ind = np.argpartition(w, -5)[-5:]\n",
        "    l_sort = l_ind[np.argsort(w[l_ind])][::-1]\n",
        "    l_weights = w[l_sort]\n",
        "    l_tuples = list(zip(l_sort, l_weights))\n",
        "    \n",
        "    print('Top 5 weights:')\n",
        "    for tup in l_tuples:\n",
        "        print('{}: {}'.format(reversed_wordlist[tup[0]], tup[1]))\n",
        "    \n",
        "    print('\\n')\n",
        "    \n",
        "    # get 5 smallest weights\n",
        "    s_ind = np.argpartition(w, 5)[:5]\n",
        "    s_sort = s_ind[np.argsort(w[s_ind])]\n",
        "    s_weights = w[s_sort]\n",
        "    s_tuples = list(zip(s_sort, s_weights))\n",
        "    \n",
        "    print('Bottom 5 weights:')\n",
        "    for tup in s_tuples:\n",
        "        print('{}: {}'.format(reversed_wordlist[tup[0]], tup[1]))\n",
        "    \n",
        "    print('\\n')\n",
        "    \n",
        "    # print bias term\n",
        "    print('Bias: {}'.format(w[0]))"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data...\n",
            "Data loaded.\n",
            "\n\n",
            "Head and tail of data:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style>\n",
              "    .dataframe thead tr:only-child th {\n",
              "        text-align: right;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599995</th>\n",
              "      <td>1</td>\n",
              "      <td>Just woke up. Having no school is the best fee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599996</th>\n",
              "      <td>1</td>\n",
              "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599997</th>\n",
              "      <td>1</td>\n",
              "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599998</th>\n",
              "      <td>1</td>\n",
              "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599999</th>\n",
              "      <td>1</td>\n",
              "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         sentiment                                              tweet\n",
              "0                0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1                0  is upset that he can't update his Facebook by ...\n",
              "2                0  @Kenichan I dived many times for the ball. Man...\n",
              "3                0    my whole body feels itchy and like its on fire \n",
              "4                0  @nationwideclass no, it's not behaving at all....\n",
              "1599995          1  Just woke up. Having no school is the best fee...\n",
              "1599996          1  TheWDB.com - Very cool to hear old Walt interv...\n",
              "1599997          1  Are you ready for your MoJo Makeover? Ask me f...\n",
              "1599998          1  Happy 38th Birthday to my boo of alll time!!! ...\n",
              "1599999          1  happy #charitytuesday @theNSPCC @SparksCharity..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n\n",
            "Number of tweets: 1600000\n",
            "Number of words: 86645\n",
            "\n\n",
            "The boolean matrix has 1600000 rows and 86646 columns.\n",
            "\n\n",
            "Pass 1 through data...\n",
            "Pass complete!\n",
            "Cost value 1: 0.5165943596188627\n",
            "\n\n",
            "Shuffling...\n",
            "\n\n",
            "Pass 2 through data...\n",
            "Pass complete!\n",
            "Cost value 2: 0.5041239183073967\n",
            "Difference in cost: 0.012470441311466018\n",
            "\n\n",
            "Top 5 weights:\n",
            "letting: 1.7264024149960315\n",
            "hows: 1.5696658721225454\n",
            "carousella: 1.4464374513979492\n",
            "till: 1.2807266501718593\n",
            "hear: 1.2235996320188314\n",
            "\n\n",
            "Bottom 5 weights:\n",
            "ooh: -2.9092679938584163\n",
            "premiere: -2.0320494453670266\n",
            "aaw: -1.8920347712339727\n",
            "spent: -1.8807150852187302\n",
            "cameron: -1.7862225131754408\n",
            "\n\n",
            "Bias: 0.27194235724608484\n"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.8.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}